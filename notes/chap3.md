# Data Parallelism and CUDA C

## Program Structure

* **host**: cpu
* **device**: gpu

A CUDA source file can have a mixture of both host and device code. A CUDA program is a traditional C program with added device functions clearly marked by special CUDA keywords. Since we've added these keywords to the source file, we can no longer feed it to a traditional C compiler. Instead, we use a CUDA C compiler by NVIDIA called (NVCC) which processes the CUDA program using the keywords to separate host and device code.

The host code is fed to the host's standard C/C++ compiler and executed as a CPU process. The device code, marked with special keywords to indicate data-parallel functions called *kernels*, is fed to the runtime compiler of NVCC and executed on the GPU.

<p align="center">
 <img src="./assets/compiler.png" alt="Drawing", width=35%>
</p>

The execution of a CUDA program starts with the execution of the host program. When a *kernel* is called or *launched*, it is executed by a large number of threads on the device. All the threads that are generated by a kernel launch are collectively called a grid. When all threads of a kernel complete their execution, the corresponding grid terminates, and the execution continues on the host until another kernel is launched.

<p align="center">
 <img src="./assets/execution.png" alt="Drawing", width=50%>
</p>

## Vector Addition Kernel

Let's start with a CPU sequential version of vector addition.

```c
// compute vector sum h_C = h_A + h_B
void vecAdd(float* h_A, float* h_B, float* h_C, int n) {
    for (i = 0; i < n; i++) {
        h_C[i] = h_A[i] + h_B[i];
    }
}
int main() {
    // memory allocation for h_A, h_B, h_C
    // initialization of h_A, h_B
    vecAdd(h_A, h_B, h_C, N); 
}
```
Note the convention of prefixing the names of variables that are processed by the host with `h_` and those of variables that are processed by the device with `d_`.

**Todo**: refresh my knowledge about pointers, referencing, etc.

The steps to modify `vecAdd` such that it can run on the device are:

* allocate host vectors + initialize them
* allocate device memory for A, B, C
* copy A and B to device memory
* kernel launch
* copy C from the device memory to the host memory
* free up resources

## Global Memory and Data Transfer

The host and the device have separate memory spaces. The device memory is referred to as the global memory. It consists in DRAM. To execute a kernel on the device, we need to allocate global memory on the device and transfer data from the host memory to the allocated device memory. Similarly, after the kernel terminates, we needs to transfer result data from the device memory back to the host memory and free up the device memory that is no longer needed.

<p align="center">
 <img src="./assets/steps.png" alt="Drawing", width=20%>
</p>

CUDA gives us API functions that perform these things for us. Let's look into them:

* `cudaMalloc()`: called from the host code to allocate a piece of device global memory for an object.
    * **address**: address of a pointer to allocated object. Should be cast to `void`.
    * **size**: size of allocated object in terms of bytes
* `cudaFree()`: frees object from device global memory
    * **pointer**: pointer to the freed object.
* `cudaMemcpy()`: memory data transfer
    * **dest**: pointer to destination
    * **src**: pointer to source
    * **size**: number of bytes copied
    * **dir**: type/direction of copy.

Here's an example of how we use each in the `vecAdd()` method of the earlier section:

```c
float *d_A = NULL;
int size = n * sizeof(float);

cudaMalloc((void **) &d_A, size);
cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
cudaFree(d_A)
```
So we have the tools to code all the steps mentioned in section 2 except for the kernel invocation. Let's go ahead and code them up.

```c
void vecAdd(float* A, float* B, float* C, int n)
{
    // params
    int size = n * sizeof(float);
    float *d_A, *d_B, *d_C;

    // allocate host vectors
    cudaMalloc((void **) &d_A, size);
    cudaMalloc((void **) &B_d, size);
    cudaMalloc((void **) &d_C, size);

    // possible initialization

    // copy A and B to device memory
    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);

    // kernel invocation code

    // copy C from device to host
    cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);

    // free up resources
    cudaFree(d_Ad); cudaFree(d_B); cudaFree (d_C);
}
```

## Kernel Functions and Threading

